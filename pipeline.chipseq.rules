import os
import io
import sys
import traceback
import json


try:
    samples = config['samples']

    data_dir = config['data_dir']
    work_dir = config['work_dir']
    pipeline_home = config['pipeline_home']
    shell.prefix("""
    set -e -o pipefail
    module purge
    sleep 20s
    MEM=`echo "${{SLURM_MEM_PER_NODE}} / 1024 "|bc`
    LOCAL="/lscratch/${{SLURM_JOBID}}/"
    THREADS=${{SLURM_CPUS_ON_NODE}}
    """)

    configfile: pipeline_home +"/config/config.chipseq.yaml"
    pipeline_version = config["common"]["pipeline_version"]
    emails = config["common"]["emails"]

    config["pipeline_home"] = pipeline_home
    config["work_dir"] = work_dir
        
    suffix_R1 = config["common"]["FASTQ_suffix_R1"]
    suffix_R2 = config["common"]["FASTQ_suffix_R2"]
    suffix_SE = config["common"]["FASTQ_suffix_SE"]

    SAMPLES = []
    #prepare targets
    FASTQS = {}
    BAMS = []
    BWS= []
    TDFS = []
    SPP = []
    MACS2 = []
    ANNOTATION = []
    ROSE = []
    MOTIFS = []
    EDENS = []
    RNAseq = []
    MACS_BAMS = {}
    for sample_id, sample in samples.items():
        SAMPLES.append(sample_id)
        FASTQS[sample_id] = []
        MACS_BAMS[sample_id] = []
        #add FASTQ targets
        if not "SampleFiles" in sample:
            raise Exception('SampleFiles not found in sample sheet')
        sample_file = sample["SampleFiles"]
        # check source FASTQ files exist and determine if this is paired_end
        if not os.path.exists(data_dir + "/" + sample_file + "/" + sample_file + suffix_R1):
            raise Exception(data_dir + "/" + sample_file + "/" + sample_file + suffix_R1 + ' not found')
        samples[sample_id]["PE"] = False
        if os.path.exists(data_dir + "/" + sample_file + "/" + sample_file + suffix_R2):
            samples[sample_id]["PE"] = True
            FASTQS[sample_id].append(sample_id + "/DATA/" + sample_id + suffix_R1)
            FASTQS[sample_id].append(sample_id + "/DATA/" + sample_id + suffix_R2)
        else:
            FASTQS[sample_id].append(sample_id + "/DATA/" + sample_id + suffix_SE)
        if not "Genome" in sample:
            raise Exception('Genome not found in sample sheet')
        if "SpikeIn" in sample and sample["SpikeIn"] == "yes":
            #only add script targets once
            if not "SpikeInGenome" in sample:
                raise Exception('SpikeInGenome not found in sample sheet')
            spike_in_genome = sample["SpikeInGenome"]
            BWS.append(sample_id + "/" + sample_id + "." + config["bin_size"] + ".scaled.bw")
            TDFS.append(sample_id + "/" + sample_id + "." + config["bin_size"] + ".scaled.tdf")
        else:
            sample["SpikeIn"] == "no"
            sample["SpikeInGenome"] == ""
        if samples[sample_id]["LibrarySize"] == ".":
            samples[sample_id]["LibrarySize"] = config["default_lib_length"]
        BAMS.append(sample_id + "/" + sample_id + ".bam")
        BAMS.append(sample_id + "/" + sample_id + ".dd.bam")
        MACS_BAMS[sample_id].append(sample_id + "/" + sample_id + ".dd.bam")
        BWS.append(sample_id + "/" + sample_id + "." + config["bin_size"] + ".RPM.bw")
        TDFS.append(sample_id + "/" + sample_id + "." + config["bin_size"] + ".RPM.tdf")
        SPP.append(sample_id + "/spp/" + sample_id + ".spp.pdf")
        SPP.append(sample_id + "/spp/" + sample_id + ".spp.txt")
        has_exp = sample["PairedRNA_SAMPLE_ID"] != "."
        if has_exp:
            RNAseq.append(sample_id + "/RNAseq/" + sample["PairedRNA_SAMPLE_ID"] + "." +  sample["Genome"] + ".ucsc.genes.TPM.txt")
        for cutoff in config["macs2"]["qvalues"]:
            peak_type = sample["PeakCalling"]
            MACS2.append(sample_id + "/MACS_Out_q_" + cutoff + "/" + sample_id + "_peaks." + peak_type + "Peak.nobl.bed")
            MACS2.append(sample_id + "/MACS_Out_q_" + cutoff + "/" + sample_id + "." + peak_type + "_summits.nobl.bed")
            if has_exp:
                EDENS.append(sample_id + "/MACS_Out_q_" + cutoff + "/" + sample_id + "_peaks." + peak_type + "Peak.nobl_TPM" + str(config["eden"]["TPM_cutoff"]) + "_multi-genes.txt")
            ANNOTATION.append(sample_id + "/MACS_Out_q_" + cutoff + "/" + sample_id + "_peaks." + peak_type + "Peak.nobl.bed.annotation.txt")
            ANNOTATION.append(sample_id + "/MACS_Out_q_" + cutoff + "/" + sample_id + "_peaks." +peak_type + "Peak.nobl.bed.annotation.summary")
            MOTIFS.append(sample_id + "/MACS_Out_q_" + cutoff + "/motif_" + peak_type + "/knownResults.html")
            if sample["EnhancePipe"] == "yes":
                ROSE.append(sample_id + "/MACS_Out_q_" + cutoff + "/ROSE_out_" + str(config["rose"]["stitch_distance"]) + "/" + sample_id + "_peaks_AllEnhancers.table.super.bed")
                ROSE.append(sample_id + "/MACS_Out_q_" + cutoff + "/ROSE_out_" + str(config["rose"]["stitch_distance"]) + "/" + sample_id + "_peaks_AllEnhancers.table.regular.bed")
                if has_exp:
                    EDENS.append(sample_id + "/MACS_Out_q_" + cutoff + "/ROSE_out_" + str(config["rose"]["stitch_distance"]) + "/" + sample_id + "_peaks_AllEnhancers.table.super_TPM" + str(config["eden"]["TPM_cutoff"]) + "_multi-genes.txt")
                    EDENS.append(sample_id + "/MACS_Out_q_" + cutoff + "/ROSE_out_" + str(config["rose"]["stitch_distance"]) + "/" + sample_id + "_peaks_AllEnhancers.table.regular_TPM" + str(config["eden"]["TPM_cutoff"]) + "_multi-genes.txt")                
        
        # if input sample not exists, generate bam file
        input_sample_id = sample["PairedInput"]
        input_sample_file = config["common"]["FASTQ_prefix"] + input_sample_id
        if input_sample_id != ".":
            input_sample = {}
            input_sample["Genome"] = sample["Genome"]
            input_sample["LibrarySize"] = sample["LibrarySize"]
            input_sample["SampleFiles"] = input_sample_file
            input_sample["SpikeIn"] = "no"
            input_sample["SpikeInGenome"] = ""
            samples[input_sample_id] = input_sample
            BAMS.append(input_sample_id + "/" + input_sample_id + ".bam")
            BAMS.append(input_sample_id + "/" + input_sample_id + ".dd.bam")
            MACS_BAMS[sample_id].append(input_sample_id + "/" + input_sample_id + ".dd.bam")
            FASTQS[input_sample_id] = []
            if os.path.exists(data_dir + "/" + input_sample_file + "/" + input_sample_file + suffix_R2):
                FASTQS[input_sample_id].append(input_sample_id + "/DATA/" + input_sample_id + suffix_R1)
                FASTQS[input_sample_id].append(input_sample_id + "/DATA/" + input_sample_id + suffix_R2)
            else:
                FASTQS[input_sample_id].append(input_sample_id + "/DATA/" + input_sample_id + suffix_SE)
        
        
except Exception as err:
    exc_type, exc_value, exc_traceback = sys.exc_info()
    output = io.StringIO()
    traceback.print_exception(exc_type, exc_value, exc_traceback, file=output)
    contents = output.getvalue()
    output.close()
    print(contents)    
    shell("echo 'ChIPseq pipeline has exception: reason " + contents + ". Working Dir:  {work_dir}' |mutt -e 'my_hdr From:chouh@nih.gov' -s 'Khanlab ChIPseq Pipeline Status' `whoami`@mail.nih.gov {emails} ")
    sys.exit()
    
onerror:
    shell("echo 'ChIPseq pipeline version {pipeline_version} failed on Biowulf. Samples: {SAMPLES}. Working Dir:  {work_dir}' |mutt -e 'my_hdr From:chouh@nih.gov' -s 'Khanlab ChIPseq Pipeline Status' `whoami`@mail.nih.gov {emails} ")
onstart:
    shell("echo 'ChIPseq pipeline version {pipeline_version} started on Biowulf. Samples: {SAMPLES}. Working Dir:  {work_dir}' |mutt -e 'my_hdr From:chouh@nih.gov' -s 'Khanlab ChIPseq Pipeline Status' `whoami`@mail.nih.gov {emails} ")
onsuccess:
    shell("echo 'ChIPseq pipeline version {pipeline_version} finished on Biowulf. Samples: {SAMPLES}. Working Dir:  {work_dir}' |mutt -e 'my_hdr From:chouh@nih.gov' -s 'Khanlab ChIPseq Pipeline Status' `whoami`@mail.nih.gov {emails} ")
    shell("for s in {SAMPLES};do touch {work_dir}/${{s}}/successful.txt;chgrp -R khanlab {work_dir}/${{s}};done")
    print("Workflow finished, no error")
    
localrules: prepareFASTQ, ChIPseq_pipeline, EDEN

rule ChIPseq_pipeline:
    input: BAMS,BWS,TDFS,SPP,MACS2,ANNOTATION,ROSE,MOTIFS,EDENS, RNAseq

rule RNAseq_pipeline:
    input:
            config["work_dir"] + "/../rnaseq"
    output:
            "{sample}/RNAseq/{rnaseq_sample}.{genome}.ucsc.genes.TPM.txt"
    version:
            config["version"]["snakemake"]
    params:
            work_dir = config["work_dir"],
            data_dir = config["data_dir"],
            now = config["now"],
            batch    = config["cluster"]["subflow"],
            pipeline_home = config["pipeline_home"],
            genome = lambda wildcards: samples[wildcards.sample]["Genome"],
            rulename = "RNAseq_pipeline",
            rnaseq_sample = lambda wildcards: samples[wildcards.sample]["PairedRNA_SAMPLE_ID"],
            log_dir = lambda wildcards: wildcards.sample + '/log',
    shell:
            """
            mkdir -p {wildcards.sample}/RNAseq
            module load python/3.6
            python {params.pipeline_home}/scripts/sampleToYaml.py -s {params.rnaseq_sample} -o {wildcards.sample}/RNAseq/{params.rnaseq_sample}.rnaseq.yaml
            module load snakemake/{version}
            snakemake --directory {params.work_dir}/../rnaseq --snakefile {params.pipeline_home}/pipeline.rnaseq.rules --configfile {wildcards.sample}/RNAseq/{params.rnaseq_sample}.rnaseq.yaml --config pipeline_home={params.pipeline_home} work_dir={params.work_dir}/../rnaseq data_dir={params.data_dir} now={params.now} \
                --jobname {{params.rulename}}.{{jobid}} --nolock  --ri -k -p -r -j 1000 --cores 150 --jobscript {params.pipeline_home}/scripts/jobscript.sh --cluster "sbatch -o \
                {{params.log_dir}}/{{params.rulename}}.%j.o -e {{params.log_dir}}/{{params.rulename}}.%j.e {{params.batch}}"
            echo -e "Chr\\tStart\\tStop\\tGeneID\\tTPM" > {output}
            join -t $'\\t' -1 5 -2 1 <(grep -v CHROMOSOME {params.pipeline_home}/ref/hg19.genes.txt|sort -k5) <(grep -v {params.work_dir}/../rnaseq/{params.rnaseq_sample}/RSEM_{params.genome}_ucsc/{params.rnaseq_sample}.{params.genome}.ucsc.genes.results|sort -k1) | awk -F'\\t' 'OFS="\\t"{{print $2,$3,$4,$1,$11}}' | sort -k 1,1 -k2,2n >> {output}
            """

rule EDEN:
    input:
            bed=lambda wildcards: wildcards.sample + "/" + wildcards.folders + "/" +  wildcards.prefix + ".bed",
            exp_file=lambda wildcards: wildcards.sample + "/RNAseq/" +  samples[wildcards.sample]["PairedRNA_SAMPLE_ID"] + "." + samples[wildcards.sample]["Genome"] + ".ucsc.genes.TPM.txt"
    output:
            "{sample}/{folders}/{prefix}_TPM{TPM_cutoff}_multi-genes.txt"
    wildcard_constraints:
            folders = "MACS.+"
    params:
            work_dir = config["work_dir"],
            batch    = config["cluster"]["small"],
            pipeline_home = config["pipeline_home"],
            tad_file = lambda wildcards: config[samples[wildcards.sample]["Genome"]]["tad_file"],
            genome = lambda wildcards: samples[wildcards.sample]["Genome"],
            super_loci_distance_cutoff = str(config["eden"]["super_loci_distance_cutoff"]),
            nearest_gene_distance_cutoff = str(config["eden"]["nearest_gene_distance_cutoff"]),
            exp_file = lambda wildcards: samples[wildcards.sample]["PairedRNA_SAMPLE_ID"] + ".gene.TPM.txt",
            rulename = "eden",
            log_dir = lambda wildcards: wildcards.sample + '/log',
    shell:
            """
            {params.pipeline_home}/scripts/EDEN.pl -e {input.exp_file} -o {wildcards.sample}/MACS_Out_{wildcards.folders} -x {wildcards.prefix} -t TPM -c -d {params.pipeline_home}/{params.tad_file} -b {input.bed} -f {wildcards.TPM_cutoff} \
                -s {params.super_loci_distance_cutoff} -n {params.nearest_gene_distance_cutoff}
            """
            
rule findMotif:
    input:            
            "{sample}/MACS_Out_q_{cutoff}/{sample}.{peak_type}_summits.nobl.bed",
    output:
            "{sample}/MACS_Out_q_{cutoff}/motif_{peak_type}/knownResults.html",
    version:
            config["version"]["homer"]
    benchmark:
            "{sample}/benchmark/findMotif.{sample}.{cutoff}.{peak_type}.benchmark.txt"
    params:
            work_dir = config["work_dir"],
            batch    = config["cluster"]["job_motif"],
            pipeline_home=config["pipeline_home"],
            genome = lambda wildcards: samples[wildcards.sample]["Genome"],
            rulename = "findMotif",
            motif_size = str(config["homer"]["motif_size"]),
            log_dir = lambda wildcards: wildcards.sample + '/log',
    shell:
            """  
            module load homer/{version}
            findMotifsGenome.pl {input} {params.genome} {wildcards.sample}/MACS_Out_q_{wildcards.cutoff}/motif_{wildcards.peak_type} -size {params.motif_size} -p ${{THREADS}} -preparsedDir {params.pipeline_home}/ref/preparsedDir
            """
rule rose:
    input:
            bed=lambda wildcards: wildcards.sample + "/MACS_Out_q_" + wildcards.cutoff + "/" + wildcards.sample + "_peaks." + samples[wildcards.sample]["PeakCalling"] + "Peak.nobl.bed",
            bam="{sample}/{sample}.dd.bam"
    output:
            rose_out="{sample}/MACS_Out_q_{cutoff}/ROSE_out_{stitch_distance}/{sample}_peaks_AllEnhancers.table.txt",
            se_table="{sample}/MACS_Out_q_{cutoff}/ROSE_out_{stitch_distance}/{sample}_peaks_AllEnhancers.table.super.bed",
            re_table="{sample}/MACS_Out_q_{cutoff}/ROSE_out_{stitch_distance}/{sample}_peaks_AllEnhancers.table.regular.bed",
    version:
            config["version"]["rose"]
    benchmark:
            "{sample}/benchmark/rose.{sample}.{cutoff}.{stitch_distance}benchmark.txt"
    params:
            work_dir = config["work_dir"],
            batch    = config["cluster"]["job_rose"],
            pipeline_home=config["pipeline_home"],
            tss_distance=config["rose"]["tss_distance"],
            genome = lambda wildcards: samples[wildcards.sample]["Genome"].upper(),
            input_control = lambda wildcards: " -c " + config["work_dir"] + "/" + samples[wildcards.sample]["PairedInput"] + "/" + samples[wildcards.sample]["PairedInput"] + ".dd.bam" if samples[wildcards.sample]["PairedInput"] != "." else "",
            rulename = "rose",
            log_dir = lambda wildcards: wildcards.sample + '/log',
    shell:
            """  
            module load rose/{version}
            module load python/2.7
            cd $(dirname `which ROSE_main.py`)
            ROSE_main.py -i {params.work_dir}/{input.bed} -g {params.genome} -r {params.work_dir}/{input.bam} {params.input_control} -t {params.tss_distance} -s {wildcards.stitch_distance} -o {params.work_dir}/{wildcards.sample}/MACS_Out_q_{cutoff}/ROSE_out_{wildcards.stitch_distance}
            {params.pipeline_home}/scripts/roseTable2Bed.sh {params.work_dir}/{output.rose_out} {params.work_dir}/{output.re_table} {params.work_dir}/{output.se_table} 
            """

rule peakAnnotation:
    input:
            "{sample}/MACS_Out_q_{cutoff}/{sample}_peaks.{peak_type}Peak.nobl.bed"
    output:
            annotation="{sample}/MACS_Out_q_{cutoff}/{sample}_peaks.{peak_type}Peak.nobl.bed.annotation.txt",
            annotation_summary="{sample}/MACS_Out_q_{cutoff}/{sample}_peaks.{peak_type}Peak.nobl.bed.annotation.summary",
    version:
            config["version"]["homer"]
    benchmark:
            "{sample}/benchmark/peakAnnotation.{sample}.{cutoff}.{peak_type}.benchmark.txt"
    params:
            work_dir = config["work_dir"],
            batch    = config["cluster"]["small"],
            pipeline_home=config["pipeline_home"],
            genome = lambda wildcards: samples[wildcards.sample]["Genome"],
            rulename = "peakAnnotation",
            log_dir = lambda wildcards: wildcards.sample + '/log',
    shell:
            """  
            module load homer/{version}
            annotatePeaks.pl {input} {params.genome} -annStats {output.annotation_summary} > {output.annotation}
            """
    
rule macs2:
    input:
            lambda wildcards: MACS_BAMS[wildcards.sample]
    output:
            peak="{sample}/MACS_Out_q_{cutoff}/{sample}_peaks.{peak_type}Peak",
            renamed_peak="{sample}/MACS_Out_q_{cutoff}/{sample}_peaks.{peak_type}Peak.bed",
            nobl="{sample}/MACS_Out_q_{cutoff}/{sample}_peaks.{peak_type}Peak.nobl.bed",
            great="{sample}/MACS_Out_q_{cutoff}/{sample}_peaks.{peak_type}Peak.nobl.GREAT.bed",
            summit="{sample}/MACS_Out_q_{cutoff}/{sample}.{peak_type}_summits.bed",
            summit_nobl="{sample}/MACS_Out_q_{cutoff}/{sample}.{peak_type}_summits.nobl.bed",
    version:
            config["version"]["macs"]
    benchmark:
            "{sample}/benchmark/macs2.{sample}.{cutoff}.{peak_type}.benchmark.txt"
    params:
            work_dir = config["work_dir"],
            batch    = config["cluster"]["job_macs2"],
            pipeline_home=config["pipeline_home"],
            genome = lambda wildcards: config[samples[wildcards.sample]["Genome"]]["macs2_gsize"],
            rulename = "macs2",
            ext_option = lambda wildcards: "" if samples[wildcards.sample]["PE"] else " --extsize " + str(samples[wildcards.sample]["LibrarySize"]),
            bam_format = lambda wildcards: " BAMPE " if samples[wildcards.sample]["PE"] else "BAM",
            peak_type = lambda wildcards: " --broad " if wildcards.peak_type=="broad" else "",
            cutoff_option = lambda wildcards: "--broad-cutoff " + wildcards.cutoff if wildcards.peak_type=="broad" else " -q " + wildcards.cutoff,
            input_control = lambda wildcards: " -c " + samples[wildcards.sample]["PairedInput"] + "/" + samples[wildcards.sample]["PairedInput"] + ".dd.bam" if samples[wildcards.sample]["PairedInput"] != "." else "",
            black_list= lambda wildcards: config["pipeline_home"] + "/" + config[samples[wildcards.sample]["Genome"]]["black_list"],
            exclude_chr_list = lambda wildcards: config["pipeline_home"] + "/" + config[samples[wildcards.sample]["Genome"]]["exclude_chr_list"],
            log_dir = lambda wildcards: wildcards.sample + '/log',
    shell:
            """  
            module load macs/{version}
            module load bedtools
            macs2 callpeak {params.peak_type} -t {wildcards.sample}/{wildcards.sample}.dd.bam {params.input_control} --nomodel  -g {params.genome} -f {params.bam_format} --name {wildcards.sample} --outdir {wildcards.sample}/MACS_Out_q_{wildcards.cutoff} {params.ext_option} {params.cutoff_option}
            #remove unused chromosomes and blacklist in peak
            {params.pipeline_home}/scripts/removeChr.pl -i {output.peak} -o {output.renamed_peak} -l {params.exclude_chr_list}
            sed -i 's/{wildcards.sample}//g' {output.renamed_peak}
            bedtools intersect -a {output.renamed_peak} -b {params.black_list} -v > {output.nobl}
            cut -f1-3 {output.nobl} > {output.great}
            #remove unused chromosomes and blacklist in summit
            {params.pipeline_home}/scripts/removeChr.pl -i {wildcards.sample}/MACS_Out_q_{wildcards.cutoff}/{wildcards.sample}_summits.bed -o {output.summit} -l {params.exclude_chr_list}
            sed -i 's/{wildcards.sample}//g' {output.summit}
            bedtools intersect -a {output.summit} -b {params.black_list} -v > {output.summit_nobl}
            """

rule crossCorrelation:
    input:
            "{sample}/{sample}.dd.bam"
    output:
            pdf="{sample}/spp/{sample}.spp.pdf",
            txt="{sample}/spp/{sample}.spp.txt"
    version:
            config["version"]["R"]
    benchmark:
            "{sample}/benchmark/crossCorrelation.{sample}.benchmark.txt"
    params:
            work_dir = config["work_dir"],
            batch    = config["cluster"]["medium"],
            pipeline_home=config["pipeline_home"],
            genome = lambda wildcards: samples[wildcards.sample]["Genome"],
            rulename = "crossCorrelation",
            log_dir = lambda wildcards: wildcards.sample + '/log',
    shell:
            """
            module load R/{version}
            module load samtools
            Rscript {params.pipeline_home}/scripts/run_spp.R -c={input} -odir={wildcards.sample}/spp -savp={output.pdf} -out={output.txt}
            """

rule makeNormTDF:
    input:
            "{sample}/{sample}.{bin_size}.bedgraph"
    output:
            "{sample}/{sample}.{bin_size}.{norm_type}.tdf"
    version:
            config["version"]["igvtools"]
    benchmark:
            "{sample}/benchmark/makeNormTDF.{sample}.{bin_size}.{norm_type}.benchmark.txt"
    params:
            work_dir = config["work_dir"],
            batch    = config["cluster"]["small"],
            pipeline_home=config["pipeline_home"],
            genome = lambda wildcards: samples[wildcards.sample]["Genome"],
            rulename = "makeNormTDF",
            log_dir = lambda wildcards: wildcards.sample + '/log',
    shell:
            """  
            module load igvtools/{version}
            if [[ "{wildcards.norm_type}" == "scaled" ]]; then
                norm_count=`tail -1 {wildcards.sample}/SpikeIn/spike_map_summary | cut -f2`
            else
                norm_count=`grep mapped {wildcards.sample}/{wildcards.sample}.dd.flagstat.txt | head -1 | cut -d' ' -f1`
            fi
            factor=`echo "1000000/${{norm_count}}" | bc -l`
            {params.pipeline_home}/scripts/scaleTDF.pl -i {input} -o {output} -f ${{factor}} -g {params.genome} -t bedgraph
            """
            
rule makeTDF:
    input:
            "{sample}/{sample}.dd.bam"
    output:
            bg="{sample}/{sample}.{bin_size}.bedgraph"
    version:
            config["version"]["igvtools"]
    benchmark:
            "{sample}/benchmark/makeTDF.{sample}.{bin_size}.benchmark.txt"
    params:
            work_dir = config["work_dir"],
            batch    = config["cluster"]["small"],
            lib_size = lambda wildcards: samples[wildcards.sample]["LibrarySize"],
            smooth_size = config["smooth_size"],            
            pair = lambda wildcards: " --pairs " if samples[wildcards.sample]["PE"] else "",
            genome = lambda wildcards: samples[wildcards.sample]["Genome"],
            rulename = "makeTDF",
            log_dir = lambda wildcards: wildcards.sample + '/log',
    shell:
            """  
            module load igvtools/{version}
            igvtools count -w {wildcards.bin_size} -e {params.lib_size} {params.pair}{input} {wildcards.sample}/{wildcards.sample}.{wildcards.bin_size}.tdf {params.genome}
            igvtools tdftobedgraph {wildcards.sample}/{wildcards.sample}.{wildcards.bin_size}.tdf {output.bg}
            """

rule makeBigWig:
    input:
            "{sample}/{sample}.dd.bam"
    output:
            "{sample}/{sample}.{bin_size}.{norm_type}.bw"
    version:
            config["version"]["deeptools"]
    benchmark:
            "{sample}/benchmark/makeBigWig.{sample}.{bin_size}.{norm_type}.benchmark.txt"
    params:
            work_dir = config["work_dir"],
            batch    = config["cluster"]["medium"],
            lib_size = lambda wildcards: samples[wildcards.sample]["LibrarySize"],
            smooth_size = config["smooth_size"],
            rulename = "makeBigWig",
            log_dir = lambda wildcards: wildcards.sample + '/log',
    shell:
            """  
            module load deeptools/{version}
            norm_param=" --normalizeUsing CPM "
            if [[ "{wildcards.norm_type}" == "scaled" ]]; then
                spike_count=`tail -1 {wildcards.sample}/SpikeIn/spike_map_summary | cut -f2`
                ref_count=`tail -1 {wildcards.sample}/SpikeIn/spike_map_summary | cut -f1`
                factor=`echo "1000000/${{spike_count}}" | bc -l`
                norm_param=" --scaleFactor ${{factor}} "
            fi
            bamCoverage -b {input} -o {output} -p ${{THREADS}} --smoothLength {params.smooth_size} --binSize {wildcards.bin_size} -e {params.lib_size} ${{norm_param}}
            
            """
rule BWA:
    input:
            lambda wildcards: FASTQS[wildcards.sample]
    output: 
            bam="{sample}/{sample}.bam",
            bai="{sample}/{sample}.bam.bai",
            dd_bam="{sample}/{sample}.dd.bam",
            dd_bai="{sample}/{sample}.dd.bam.bai"
    version:
            config["version"]["bwa"]
    params:
            bwa_idx = lambda wildcards: config[samples[wildcards.sample]["Genome"]]["bwa_index"] + ".fa" if samples[wildcards.sample]["SpikeIn"] != "yes" else config[samples[wildcards.sample]["Genome"]]["bwa_index"] + "." + samples[wildcards.sample]["SpikeInGenome"] + ".fa",
            fastqs = lambda wildcards: " ".join(work_dir + '/' + x for x in FASTQS[wildcards.sample]),
            work_dir = config["work_dir"],
            batch    = config["cluster"]["job_bwa"],
            rulename = "BWA",
            genome = lambda wildcards: samples[wildcards.sample]["Genome"],
            spikeIn = lambda wildcards: samples[wildcards.sample]["SpikeIn"],
            spikeInGenome = lambda wildcards: samples[wildcards.sample]["SpikeInGenome"],
            min_mapq = config["min_mapq"],            
            log_dir = lambda wildcards: wildcards.sample + '/log',
    benchmark:
            "{sample}/benchmark/bwa.{sample}.benchmark.txt"
    shell:
            """  
            module load bwa/{version}
            module load samtools
            cd ${{LOCAL}}
            bwa mem -t ${{THREADS}} {params.bwa_idx} {params.fastqs} | samtools view -bhq {params.min_mapq} | samtools sort -@ ${{THREADS}} -o {wildcards.sample}.bam -O bam -
            samtools index -@ ${{THREADS}} {wildcards.sample}.bam
            module load picard
            module load deeptools/3.4.2
            mkdir -p {wildcards.sample}
            if [[ "{params.spikeIn}" == "yes" ]];then
                for c in `samtools idxstats {wildcards.sample}.bam | cut -f1 | grep -v '*'`;do 
                    samtools view {wildcards.sample}.bam ${{c}} -b > {wildcards.sample}.${{c}}.bam &
                done
                wait
                echo "done split finished"
                mkdir -p {params.work_dir}/{wildcards.sample}/SpikeIn
                samtools merge -@ ${{THREADS}} -f {wildcards.sample}.{params.spikeInGenome}.bam {wildcards.sample}.{params.spikeInGenome}_*.bam
                samtools merge -@ ${{THREADS}} -f {wildcards.sample}.{params.genome}.bam {wildcards.sample}.chr*.bam
                samtools index -@ ${{THREADS}} {wildcards.sample}.{params.genome}.bam
                samtools index -@ ${{THREADS}} {wildcards.sample}.{params.spikeInGenome}.bam
                java -jar $PICARDJARPATH/picard.jar MarkDuplicates AS=true M={wildcards.sample}.{params.spikeInGenome}.metrix.txt I={wildcards.sample}.{params.spikeInGenome}.bam O={wildcards.sample}.{params.spikeInGenome}.dd.bam REMOVE_DUPLICATES=false VALIDATION_STRINGENCY=SILENT
                java -jar $PICARDJARPATH/picard.jar MarkDuplicates AS=true M={wildcards.sample}.{params.genome}.metrix.txt I={wildcards.sample}.{params.genome}.bam O={wildcards.sample}.{params.genome}.dd.bam REMOVE_DUPLICATES=false VALIDATION_STRINGENCY=SILENT
                samtools index -@ ${{THREADS}} {wildcards.sample}.{params.spikeInGenome}.dd.bam
                samtools index -@ ${{THREADS}} {wildcards.sample}.{params.genome}.dd.bam
                samtools flagstat -@ ${{THREADS}} {wildcards.sample}.{params.spikeInGenome}.dd.bam > {wildcards.sample}.{params.spikeInGenome}.dd.flagstat.txt
                samtools flagstat -@ ${{THREADS}} {wildcards.sample}.{params.genome}.dd.bam > {wildcards.sample}.{params.genome}.dd.flagstat.txt
                mapped_spikeIn=`grep mapped {wildcards.sample}.{params.spikeInGenome}.dd.flagstat.txt | head -1 | cut -d' ' -f1`
                mapped_ref=`grep mapped {wildcards.sample}.{params.genome}.dd.flagstat.txt | head -1 | cut -d' ' -f1`
                echo -e "{params.genome}\t{params.spikeInGenome}\n${{mapped_ref}}\t${{mapped_spikeIn}}" > {params.work_dir}/{wildcards.sample}/SpikeIn/spike_map_summary
                mv {wildcards.sample}.{params.spikeInGenome}.* {params.work_dir}/{wildcards.sample}/SpikeIn/
                mv {wildcards.sample}.{params.genome}.bam {params.work_dir}/{wildcards.sample}/{wildcards.sample}.bam
                mv {wildcards.sample}.{params.genome}.bam.bai {params.work_dir}/{wildcards.sample}/{wildcards.sample}.bam.bai
                
            else
                java -jar $PICARDJARPATH/picard.jar MarkDuplicates AS=true M={wildcards.sample}.{params.genome}.metrix.txt I={wildcards.sample}.bam O={wildcards.sample}.{params.genome}.dd.bam REMOVE_DUPLICATES=false VALIDATION_STRINGENCY=SILENT                                
                samtools index -@ ${{THREADS}} {wildcards.sample}.{params.genome}.dd.bam
                samtools flagstat -@ ${{THREADS}} {wildcards.sample}.{params.genome}.dd.bam > {wildcards.sample}.{params.genome}.dd.flagstat.txt
                mv {wildcards.sample}.bam {params.work_dir}/{wildcards.sample}
                mv {wildcards.sample}.bam.bai {params.work_dir}/{wildcards.sample}
            fi
            #move output files            
            mv {wildcards.sample}.{params.genome}.dd.bam {params.work_dir}/{wildcards.sample}/{wildcards.sample}.dd.bam
            mv {wildcards.sample}.{params.genome}.dd.bam.bai {params.work_dir}/{wildcards.sample}/{wildcards.sample}.dd.bam.bai
            mv {wildcards.sample}.{params.genome}.metrix.txt {params.work_dir}/{wildcards.sample}/{wildcards.sample}.metrix.txt
            mv {wildcards.sample}.{params.genome}.dd.flagstat.txt {params.work_dir}/{wildcards.sample}/{wildcards.sample}.dd.flagstat.txt
            
            """

rule prepareFASTQ:
    input: 
            lambda wildcards: data_dir + "/" + samples[wildcards.sample]["SampleFiles"] + "/" + samples[wildcards.sample]["SampleFiles"] + "_" + wildcards.suffix
    output: 
            "{sample}/DATA/{sample}_{suffix}",
    shell:
            """
            mkdir -p {wildcards.sample}
            mkdir -p {wildcards.sample}/log
            mkdir -p {wildcards.sample}/DATA
            ln -s {input} {output}
            """